import os
import re
from glob import glob
from typing import List, Tuple, Optional

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from collections import OrderedDict
import json
from pathlib import Path


def list_csv_files(root_dir: str) -> List[str]:
    # Search recursively for any CSV files under the dataset root. Users will
    # place dataset folders under `datasets/` so this will find them regardless
    # of subfolder layout.
    pattern = os.path.join(root_dir, '**', '*.csv')
    files = glob(pattern, recursive=True)
    # Filter out common non-data files (like MANIFEST/TXT) by extension only
    csvs = [f for f in files if f.lower().endswith('.csv')]
    return csvs


class MultimodalWindowDataset(Dataset):
    """Loads sliding windows from per-person CSVs or NPZ shards.

    Backend supports 'csv' (default) or 'npz'. For 'npz', the directory should
    contain .npz files generated by `scripts/convert_to_npz.py` with arrays:
      - pose (N, F), traj (N,2), label (N,), future_label (N,)

    Windows are fixed-length (seq_len) sampled with stride.
    """

    def __init__(self, root_dir: str, seq_len: int = 30, stride: int = 15, files: Optional[List[str]] = None, backend: str = 'csv'):
        self.root = root_dir
        self.seq_len = seq_len
        self.stride = stride
        self.backend = backend
        # load metadata (traj mean/std) if available (for npz backend)
        self.traj_mean = None
        self.traj_std = None
        try:
            from pathlib import Path as _Path
            meta_path = _Path(root_dir) / 'metadata.json'
            if meta_path.exists():
                with open(meta_path, 'r') as fh:
                    meta = json.load(fh)
                if 'traj_mean' in meta and 'traj_std' in meta:
                    self.traj_mean = np.array(meta['traj_mean'], dtype=np.float32)
                    self.traj_std = np.array(meta['traj_std'], dtype=np.float32)
                    print('Loaded traj normalization from', str(meta_path))
        except Exception:
            # ignore metadata load failures
            self.traj_mean = None
            self.traj_std = None
        if files is not None:
            self.files = files
        else:
            if backend == 'npz':
                from pathlib import Path
                self.files = [str(p) for p in Path(root_dir).rglob('*.npz')]
            else:
                self.files = list_csv_files(root_dir)

        # Instead of building a potentially huge per-window index in memory,
        # we store per-file window counts and map a global idx -> (file, start)
        self.file_infos = []  # list of dicts: {'file': path, 'n_rows': n, 'n_windows': w}
        self.cum_counts = [0]

        # small cache to avoid re-loading large files repeatedly
        self._df_cache = OrderedDict()
        self._df_cache_max = 8

        self._inspect_files()

    def _inspect_files(self):
        for f in self.files:
            try:
                if self.backend == 'npz' and str(f).lower().endswith('.npz'):
                    # For npz shards, load header arrays to get length without pandas
                    try:
                        arr = np.load(f)
                        if 'pose' in arr:
                            n = int(arr['pose'].shape[0])
                        elif 'traj' in arr:
                            n = int(arr['traj'].shape[0])
                        else:
                            # unknown npz layout
                            continue
                    except Exception:
                        continue
                else:
                    # fast row count without loading full CSV into pandas
                    with open(f, 'r', encoding='utf-8', errors='ignore') as fh:
                        n_lines = sum(1 for _ in fh)
                        n = max(0, n_lines - 1)
            except Exception:
                # fallback to pandas if we can't open raw file for some reason
                try:
                    df = pd.read_csv(f, low_memory=False)
                    n = len(df)
                except Exception:
                    continue

            if n < self.seq_len:
                continue
            # number of windows in this file
            n_windows = ((n - self.seq_len) // self.stride) + 1
            self.file_infos.append({'file': f, 'n_rows': n, 'n_windows': n_windows})
            self.cum_counts.append(self.cum_counts[-1] + n_windows)

        if len(self.file_infos) == 0:
            raise RuntimeError('No windows found in dataset. Check CSV paths and seq_len.')

    def _read_window(self, file_path: str, start: int) -> Tuple[np.ndarray, np.ndarray, int, int]:
        # handle npz backend separately for fast random access
        if self.backend == 'npz' and str(file_path).lower().endswith('.npz'):
            try:
                arr = np.load(file_path)
            except Exception:
                # empty fallback
                pose = np.zeros((self.seq_len, 0), dtype=np.float32)
                traj = np.zeros((self.seq_len, 2), dtype=np.float32)
                return pose, traj, 0, 0
            
            # Note: pose_type handling removed - model handles dimensions dynamically
                # Infer keypoint type from 72-dim pose
                pose_type = 'keypoint'

            # load arrays safely
            if 'pose' in arr:
                p = arr['pose']
                if p.shape[0] >= start + self.seq_len:
                    pose = p[start:start + self.seq_len]
                else:
                    # pad if missing rows
                    pad_n = max(0, start + self.seq_len - p.shape[0])
                    slice_p = p[start:p.shape[0]] if start < p.shape[0] else np.zeros((0, p.shape[1]))
                    pose = np.vstack([slice_p, np.zeros((pad_n, p.shape[1]))]) if slice_p.size else np.zeros((self.seq_len, p.shape[1]))
            else:
                pose = np.zeros((self.seq_len, 0), dtype=np.float32)

            if 'traj' in arr:
                t = arr['traj']
                if t.shape[0] >= start + self.seq_len:
                    traj = t[start:start + self.seq_len]
                else:
                    pad_n = max(0, start + self.seq_len - t.shape[0])
                    slice_t = t[start:t.shape[0]] if start < t.shape[0] else np.zeros((0, t.shape[1]))
                    traj = np.vstack([slice_t, np.zeros((pad_n, t.shape[1]))]) if slice_t.size else np.zeros((self.seq_len, t.shape[1]))
            else:
                traj = np.zeros((self.seq_len, 2), dtype=np.float32)

            # normalize traj if metadata present and shapes match
            try:
                if (self.traj_mean is not None) and (self.traj_std is not None) and (traj.shape[1] == self.traj_mean.shape[0]):
                    std_safe = np.array(self.traj_std, dtype=float)
                    std_safe[std_safe <= 0] = 1.0
                    traj = (traj - np.array(self.traj_mean, dtype=float).reshape(1, -1)) / std_safe.reshape(1, -1)
            except Exception:
                # be conservative: if normalization fails, keep original traj
                pass

            # labels may be stored per-frame. Prefer future_label (will_interact) as the
            # window target because processed files were trimmed to pre-interaction frames.
            if 'future_label' in arr:
                futs = arr['future_label']
                slice_fut = futs[start:start + self.seq_len] if futs.shape[0] >= start else np.zeros((0,))
                future_label = int(bool(slice_fut.any()))
            else:
                future_label = 0

            # main label: prefer future_label, fallback to interacting label if no future info
            if future_label:
                label = 1
            else:
                if 'label' in arr:
                    lbls = arr['label']
                    slice_lbl = lbls[start:start + self.seq_len] if lbls.shape[0] >= start else np.zeros((0,))
                    label = int(bool(slice_lbl.any()))
                else:
                    label = 0

            return pose.astype(np.float32), traj.astype(np.float32), label, future_label

        # CSV / pandas path (existing behavior)
        # use cached DataFrame when available to avoid repeated disk reads
        df = self._df_cache.get(file_path)
        if df is None:
            try:
                df = pd.read_csv(file_path, low_memory=False)
            except Exception:
                # if reading fails, create an empty DataFrame with no rows
                df = pd.DataFrame()
            # maintain small LRU cache
            self._df_cache[file_path] = df
            if len(self._df_cache) > self._df_cache_max:
                self._df_cache.popitem(last=False)

        if df.shape[0] == 0:
            win = pd.DataFrame()
        else:
            win = df.iloc[start:start + self.seq_len].reset_index(drop=True)

        # pose features: collect columns that end with _x or _y
        cols = [c for c in win.columns if re.search(r'(_x|_y)$', c)]
        cols = sorted(cols)
        if len(cols) == 0:
            pose = np.zeros((self.seq_len, 0), dtype=np.float32)
        else:
            pose = win[cols].fillna(0.0).to_numpy(dtype=np.float32)
            # flatten ordering is [kp1_x, kp1_y, kp2_x, kp2_y,...]

        # trajectory: prefer pelvis_x/pelvis_y
        if 'pelvis_x' in win.columns and 'pelvis_y' in win.columns:
            traj = win[['pelvis_x', 'pelvis_y']].fillna(0.0).to_numpy(dtype=np.float32)
        elif 'cart_x' in win.columns and 'cart_y' in win.columns:
            traj = win[['cart_x', 'cart_y']].fillna(0.0).to_numpy(dtype=np.float32)
        else:
            traj = np.zeros((self.seq_len, 2), dtype=np.float32)

        # auxiliary label: future_interaction (if present)
        if 'future_interaction' in win.columns:
            futs = win['future_interaction'].fillna(False).astype(bool).to_numpy()
            future_label = int(bool(futs.any()))
        elif 'will_interact' in win.columns:
            futs = win['will_interact'].fillna(False).astype(bool).to_numpy()
            future_label = int(bool(futs.any()))
        else:
            future_label = 0

        # main label: prefer future_label (will_interact) as window target; fall back to
        # 'interacting' if no future info is available.
        if future_label:
            label = 1
        else:
            if 'interacting' in win.columns:
                lbls = win['interacting'].fillna(False).astype(bool).to_numpy()
                label = int(bool(lbls.any()))
            else:
                label = 0
        
        # CSV files are keypoint-based (72-dim)
        # Note: pose_type no longer used by model but kept for logging
        
        return pose, traj, label, future_label
    
    def __len__(self):
        # total windows across files
        if len(self.file_infos) == 0:
            return 0
        return self.cum_counts[-1]

    def __getitem__(self, idx: int):
        # map global idx -> file and start
        if idx < 0:
            idx = len(self) + idx
        # binary search in cum_counts to find file index
        import bisect
        file_idx = bisect.bisect_right(self.cum_counts, idx) - 1
        if file_idx < 0:
            raise IndexError(idx)
        local_idx = idx - self.cum_counts[file_idx]
        start = local_idx * self.stride
        file_path = self.file_infos[file_idx]['file']
        pose, traj, label, future_label = self._read_window(file_path, start)
        sample = {
            'pose': torch.from_numpy(pose),
            'traj': torch.from_numpy(traj),
            'has_pose': torch.tensor(1.0) if pose.shape[1] > 0 else torch.tensor(0.0),
            'has_traj': torch.tensor(1.0) if traj.shape[1] > 0 else torch.tensor(0.0),
            'label': torch.tensor(label, dtype=torch.long),
            'future_label': torch.tensor(future_label, dtype=torch.long),
        }
        return sample


def collate_fn(batch: List[dict]):
    """Collate function for DataLoader.
    
    Model now handles pose dimensions dynamically, so we just pad to max dimension in batch.
    """
    pose_list = [b['pose'] for b in batch]
    traj_list = [b['traj'] for b in batch]
    has_pose = torch.stack([b['has_pose'] for b in batch]).float()
    has_traj = torch.stack([b['has_traj'] for b in batch]).float()
    labels = torch.stack([b['label'] for b in batch])
    future_labels = torch.stack([b.get('future_label', torch.tensor(0)) for b in batch])

    # pad pose to max dims if needed (some files might have different keypoint counts)
    max_pose_dim = max([p.shape[1] for p in pose_list])
    if max_pose_dim == 0:
        pose_tensor = torch.zeros(len(batch), pose_list[0].shape[0], 0)
    else:
        T = pose_list[0].shape[0]
        pose_tensor = torch.zeros(len(batch), T, max_pose_dim, dtype=torch.float32)
        for i, p in enumerate(pose_list):
            if p.shape[1] == 0:
                continue
            pose_tensor[i, :, :p.shape[1]] = p

    traj_tensor = torch.stack(traj_list).float()

    return {
        'pose': pose_tensor,
        'traj': traj_tensor,
        'has_pose': has_pose,
        'has_traj': has_traj,
        'label': labels,
        'future_label': future_labels,
    }
